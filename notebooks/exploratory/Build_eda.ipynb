{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y_igj9KErxgJ"
   },
   "outputs": [],
   "source": [
    "#install many of the necessary libraries. Will typically get an error regarding \"pillow\". Just re run cell\n",
    "import solaris as sol\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import descartes\n",
    "from matplotlib import pyplot as plt\n",
    "from pathlib import Path\n",
    "import rasterio\n",
    "import os\n",
    "import supermercado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the file paths for the first selected COG and labels:\n",
    "region = 'acc'\n",
    "zone = '665946'\n",
    "train_directory = 'train_tier_1'\n",
    "\n",
    "geojson = f'../../data/raw/{train_directory}/{region}/{zone}-labels/{zone}.geojson'\n",
    "geotif = f'../../data/raw/{train_directory}/{region}/{zone}/{zone}.tif'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make the directories to store processed/output data\n",
    "data_dir = '../../data/processed/'\n",
    "!mkdir ../../data/processed/images-256-{region}-{zone}\n",
    "img_path = '../../data/processed/images-256'\n",
    "!mkdir ../../data/processed/masks-256-{region}-{zone}\n",
    "mask_path = '../../data/processed/masks-256'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UftE60AWml4i"
   },
   "outputs": [],
   "source": [
    "# load geojson for labels\n",
    "\n",
    "label_df = gpd.read_file(geojson)\n",
    "# remove empty rows\n",
    "# label_df = label_df[label_df['geometry'].isna() != True] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 609
    },
    "colab_type": "code",
    "id": "-6b56_XJn2NT",
    "outputId": "e6f410b8-4f7b-4edd-f949-64814639c9fa"
   },
   "outputs": [],
   "source": [
    "#visualize the polygons\n",
    "label_df.plot(figsize=(10,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creat training and validation subsets of the geometry\n",
    "\n",
    "We need the polygons and the geotif imagery to be clipped into small tiles in order to be processes and eventually fed into a a CNN. To begin with we need to take the following steps:\n",
    "\n",
    "1. Using supermercado tile burn method to create square polygon \"cells\" representing all the slippy map tiles at a specified zoom level that overlap the labeled polygons. \n",
    "2. Then we seperate these cells into training and validation sets. \n",
    "\n",
    "\n",
    "### To do: \n",
    "split into the train/val sets more efficiently (i.e. not making new dfs, then re combing them.) Instead, label 20% of them as \"validation\" in new column \"dataset\" and that way is not just the southern portion of the region that is validation and the norther part that is training because this type of thing could lead to too variant senstive model. If intead the validation cells are sprinkled throughout the entire region, it will be more evenly represented and not affected by geographic interdependencies. For example in the region visualized above there are clearly smaller and more dense buildings in the southern region, and larger more sparse buildings in the northern region.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the tile size and zoom level for the burned tiles\n",
    "zoom_level = 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This bash command uses the supermercado burn tiles method to creat a new geojson of the tiles \n",
    "!cat ../../data/raw/train_tier_1/acc/665946/665946.json | supermercado burn {zoom_level} | mercantile shapes | fio collect > ../../data/raw/train_tier_1/acc/665946/trn_aoi_ztiles.geojson\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the geojson containing the tiles\n",
    "trn_tiles = gpd.read_file(\"../../data/raw/train_tier_1/acc/665946/trn_aoi_ztiles.geojson\")\n",
    "trn_tiles.plot(figsize=(10,10), color='grey', alpha=0.5, edgecolor='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the bounds of the supermercado cells\n",
    "bounds = trn_tiles.bounds\n",
    "\n",
    "#find the coordinates for 70/30 split on y axis extent\n",
    "extent = (bounds.maxy.max()-bounds.miny.min())*.7\n",
    "\n",
    "#create the y coodinates for training and validation subsets\n",
    "train_ymin = bounds.maxy.max()\n",
    "train_ymax = bounds.maxy.max()-extent\n",
    "val_ymax = bounds.maxy.max()-extent\n",
    "val_ymin = bounds.miny.min()\n",
    "\n",
    "#use indexing to subset the training area\n",
    "train_tiles = trn_tiles.cx[:, train_ymin:train_ymax]\n",
    "train_tiles.plot(figsize=(10, 3));\n",
    "\n",
    "#use indexing to subset the validation area\n",
    "val_tiles = trn_tiles.cx[:,val_ymin : val_ymax]\n",
    "val_tiles.plot(figsize=(10, 3));\n",
    "\n",
    "#create a \"dataset\" column for each subset\n",
    "val_tiles['dataset']=\"validation\"\n",
    "train_tiles['dataset']=\"train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see if there's overlapping tiles between trn and val\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "val_tiles.plot(ax=ax, color='grey', alpha=0.5, edgecolor='red')\n",
    "train_tiles.plot(ax=ax, color='grey', alpha=0.5, edgecolor='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge into one gdf to keep all trn tiles while dropping overlapping/duplicate val tiles\n",
    "\n",
    "tiles_gdf = gpd.GeoDataFrame(pd.concat([train_tiles, val_tiles], ignore_index=True), crs=trn_tiles.crs)\n",
    "tiles_gdf.drop_duplicates(subset=['id'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that there's no more overlapping tiles between trn and val and also project the label polygons\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "tiles_gdf[tiles_gdf['dataset'] == 'train'].plot(ax=ax, color='grey', edgecolor='red', alpha=0.3)\n",
    "tiles_gdf[tiles_gdf['dataset'] == 'validation'].plot(ax=ax, color='grey', edgecolor='blue', alpha=0.3)\n",
    "label_df.plot(ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert 'id' string to list of ints for z,x,y\n",
    "\n",
    "def reformat_xyz(tile_gdf):\n",
    "  tile_gdf['xyz'] = tile_gdf.id.apply(lambda x: x.lstrip('(,)').rstrip('(,)').split(','))\n",
    "  tile_gdf['xyz'] = [[int(q) for q in p] for p in tile_gdf['xyz']]\n",
    "  return tile_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use above funciton to create xyz column\n",
    "tiles_gdf = reformat_xyz(tiles_gdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load/crop COGtiff  to the area of one of the burned tiles using rio_tiler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rio_tiler import main as rt_main\n",
    "\n",
    "import mercantile\n",
    "from rasterio.transform import from_bounds\n",
    "from shapely.geometry import Polygon\n",
    "from shapely.ops import cascaded_union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define tile size. here we will use 256x256 pixel \n",
    "tile_size = 256\n",
    "\n",
    "#Choose a random tile's id:\n",
    "idx = 381\n",
    "\n",
    "#clip the tile\n",
    "tile, mask = rt_main.tile(geotif, *tiles_gdf.iloc[idx]['xyz'], tilesize=tile_size)\n",
    "\n",
    "#display the clipped COGtif\n",
    "plt.imshow(np.moveaxis(tile,0,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crop the label polygons to the area of the same tile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preemptively fix and merge any invalid or overlapping geoms that would otherwise throw errors during the rasterize step. \n",
    "# TODO: probably a better way to do this\n",
    "\n",
    "# https://gis.stackexchange.com/questions/271733/geopandas-dissolve-overlapping-polygons\n",
    "# https://nbviewer.jupyter.org/gist/rutgerhofste/6e7c6569616c2550568b9ce9cb4716a3\n",
    "\n",
    "def explode(gdf):\n",
    "    \"\"\"    \n",
    "    Will explode the geodataframe's muti-part geometries into single \n",
    "    geometries. Each row containing a multi-part geometry will be split into\n",
    "    multiple rows with single geometries, thereby increasing the vertical size\n",
    "    of the geodataframe. The index of the input geodataframe is no longer\n",
    "    unique and is replaced with a multi-index. \n",
    "\n",
    "    The output geodataframe has an index based on two columns (multi-index) \n",
    "    i.e. 'level_0' (index of input geodataframe) and 'level_1' which is a new\n",
    "    zero-based index for each single part geometry per multi-part geometry\n",
    "    \n",
    "    Args:\n",
    "        gdf (gpd.GeoDataFrame) : input geodataframe with multi-geometries\n",
    "        \n",
    "    Returns:\n",
    "        gdf (gpd.GeoDataFrame) : exploded geodataframe with each single \n",
    "                                 geometry as a separate entry in the \n",
    "                                 geodataframe. The GeoDataFrame has a multi-\n",
    "                                 index set to columns level_0 and level_1\n",
    "        \n",
    "    \"\"\"\n",
    "    gs = gdf.explode()\n",
    "    gdf2 = gs.reset_index().rename(columns={0: 'geometry'})\n",
    "    gdf_out = gdf2.merge(gdf.drop('geometry', axis=1), left_on='level_0', right_index=True)\n",
    "    gdf_out = gdf_out.set_index(['level_0', 'level_1']).set_geometry('geometry')\n",
    "    gdf_out.crs = gdf.crs\n",
    "    return gdf_out\n",
    "\n",
    "def cleanup_invalid_geoms(all_polys):\n",
    "  all_polys_merged = gpd.GeoDataFrame()\n",
    "  all_polys_merged['geometry'] = gpd.GeoSeries(cascaded_union([p.buffer(0) for p in all_polys]))\n",
    "\n",
    "  gdf_out = explode(all_polys_merged)\n",
    "  gdf_out = gdf_out.reset_index()\n",
    "  gdf_out.drop(columns=['level_0','level_1'], inplace=True)\n",
    "  all_polys = gdf_out['geometry']\n",
    "  return all_polys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the geometries from the geodataframe\n",
    "all_polys = label_df.geometry\n",
    "\n",
    "all_polys = cleanup_invalid_geoms(all_polys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the same tile polygon as our tile image above\n",
    "tile_poly = tiles_gdf.iloc[idx]['geometry']\n",
    "print(tile_poly.bounds)\n",
    "tile_poly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get affine transformation matrix for this tile using rasterio.transform.from_bounds: \n",
    "\n",
    "\n",
    "\"\"\" Affine transformation is a linear mapping method that preserves points, \n",
    "    straight lines, and planes. Sets of parallel lines remain parallel after an affine transformation. \n",
    "    The affine transformation technique is typically used to correct for geometric distortions or deformations that       occur with non-ideal camera angles\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfm = from_bounds(*tile_poly.bounds, tile_size, tile_size) \n",
    "tfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_polys = [poly for poly in all_polys if poly.intersects(tile_poly)]\n",
    "cropped_polys_gdf = gpd.GeoDataFrame(geometry=cropped_polys, crs={'init': 'epsg:4326'})\n",
    "cropped_polys_gdf.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Use solaris to create a pixel mask\n",
    "\n",
    "We'll create our corresponding 3-channel RGB mask by passing the cropped polygons to solaris' df_to_px_mask function. \n",
    "\n",
    "- 1st (Red) channel represent building footprints,\n",
    "- 2nd (Green) channel represent building boundaries (visually looks yellow on the RGB mask display because the pixels overlap red and green+red=yellow),\n",
    "-  3rd (Blue) channel represent close contact points between adjacent buildings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# burn a footprint/boundary/contact 3-channel mask with solaris: https://solaris.readthedocs.io/en/latest/tutorials/notebooks/api_masks_tutorial.html\n",
    "\n",
    "fbc_mask = sol.vector.mask.df_to_px_mask(df=cropped_polys_gdf,\n",
    "                                         channels=['footprint', 'boundary', 'contact'],\n",
    "                                         affine_obj=tfm, shape=(tile_size,tile_size),\n",
    "                                         boundary_width=5, boundary_type='inner', contact_spacing=5, meters=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize the new mask next to the COGtiff tile\n",
    "fig, (ax1, ax2) = plt.subplots(1,2,figsize=(10, 5))\n",
    "ax1.imshow(np.moveaxis(tile,0,2))\n",
    "ax2.imshow(fbc_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize the new mask overlaying COGtiff tile\n",
    "fig, ax = plt.subplots( figsize=(10,10))\n",
    "ax.imshow(np.moveaxis(tile,0,2))\n",
    "ax.imshow(fbc_mask, alpha = .3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1,3,figsize=(10, 5))\n",
    "ax1.imshow(fbc_mask[:,:,0])\n",
    "ax2.imshow(fbc_mask[:,:,1])\n",
    "ax3.imshow(fbc_mask[:,:,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clip all of the aerial imagery cells, clip all of the polygons, and rasterize all of the labels \n",
    "\n",
    "Combine the above process into functions in order to batch process the entire regeion. This will generate the full training and validation sets of Slippy tile cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_tile_img(tif_url, xyz, tile_size, save_path='', prefix='', display=False):\n",
    "  x,y,z = xyz\n",
    "  tile, mask = rt_main.tile(tif_url, x,y,z, tilesize=tile_size)\n",
    "  if display: \n",
    "    plt.imshow(np.moveaxis(tile,0,2))\n",
    "    plt.show()\n",
    "    \n",
    "  skimage.io.imsave(f'{save_path}/{prefix}{z}_{x}_{y}.png',np.moveaxis(tile,0,2), check_contrast=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_tile_mask(labels_poly, tile_poly, xyz, tile_size, save_path='', prefix='', display=False):\n",
    "  x,y,z = xyz\n",
    "  tfm = from_bounds(*tile_poly.bounds, tile_size, tile_size) \n",
    "  \n",
    "  cropped_polys = [poly for poly in labels_poly if poly.intersects(tile_poly)]\n",
    "  cropped_polys_gdf = gpd.GeoDataFrame(geometry=cropped_polys, crs={'init': 'epsg:4326'})\n",
    "  \n",
    "  fbc_mask = sol.vector.mask.df_to_px_mask(df=cropped_polys_gdf,\n",
    "                                         channels=['footprint', 'boundary', 'contact'],\n",
    "                                         affine_obj=tfm, shape=(tile_size,tile_size),\n",
    "                                         boundary_width=5, boundary_type='inner', contact_spacing=5, meters=True)\n",
    "  \n",
    "  if display: plt.imshow(fbc_mask); plt.show()\n",
    "  \n",
    "  skimage.io.imsave(f'{save_path}/{prefix}{z}_{x}_{y}_mask.png',fbc_mask, check_contrast=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the # of tiles that should result from each set:\n",
    "tiles_gdf[tiles_gdf['dataset'] == 'train'].shape, tiles_gdf[tiles_gdf['dataset'] == 'validation'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: try loading from url and catch i/o exceptions\n",
    "\n",
    "\n",
    "#Functionalize this so I can iterate through all regions \n",
    "\n",
    "#TODO: When training model on WHOLE dataset (all regions) load from URL, batch process (clip all of the tiles), save\n",
    "#the tiles to Google Drive, then dump the COG and the original label geometry so that I don't have to store both the raw and the processed data. \n",
    "\n",
    "#cool things to remember about this loop:\n",
    "    #1. tqdm creats a progress bar for the loop\n",
    "    #2. .iterrows makes it easy to iterate through a Dataframe\n",
    "    \n",
    "\n",
    "#remember: geotif, img_path, region, zone are defined at the top of notebook when data is first imported\n",
    "    \n",
    "for idx, tile in tqdm(tiles_gdf.iterrows()):\n",
    "    dataset = tile['dataset']\n",
    "    save_tile_img(geotif, tile['xyz'], tile_size, save_path=img_path, prefix=f'{region}{zone}{dataset}_', display=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the save_tile_mask fucntion throws a bunch of warnings. figure out how to surpress these warnings using:\n",
    "#warnings.filterwarnings(\"ignore\")s.simplefilter(\"ignore\")\n",
    "#somewhere in the save_tile_mask function \n",
    "\n",
    "\n",
    "# TODO: multiprocess this? \n",
    "for idx, tile in tqdm(tiles_gdf.iterrows()):\n",
    "  dataset = tile['dataset']\n",
    "  tile_poly = tile['geometry']\n",
    "  save_tile_mask(all_polys, tile_poly, tile['xyz'], tile_size, save_path=mask_path,prefix=f'{region}{zone}{dataset}_', display=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check that the labels and COG cells saved correctly and that the label masks are burned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use glob to put all of the file names into a dataframe for easy access \n",
    "import glob\n",
    "\n",
    "ims = glob.glob('../../data/processed/images-256/*.png')\n",
    "df_filepaths = pd.DataFrame({\n",
    "    'img_path':ims,\n",
    "    'mask_path':[im.replace('images', 'masks').replace('.png', '_mask.png') for im in ims]\n",
    "})\n",
    "df_filepaths.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterate through the dataframe and visualize a few cells and masks\n",
    "\n",
    "start_idx, end_idx = 200,205 #choose random 5 index numbers to check \n",
    "\n",
    "for i in range(start_idx, end_idx):\n",
    "    image, mask = df_filepaths.iloc[i]\n",
    "    fig, (ax1,ax2) = plt.subplots(1,2,figsize=(10,5))\n",
    "    ax1.imshow(skimage.io.imread(image))\n",
    "    ax2.imshow(skimage.io.imread(mask))\n",
    "    plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE: the above region's total cells make up ~53 mb of data: 49 from the COG tiles and 3 from the masks. This makes\n",
    "# it seem way more possible to just have everything on my local machine. \n",
    "\n",
    "#That being said, can mount drive and save all of the tiles to google drive if run the following:\n",
    "\n",
    "# # compress and download\n",
    "# !tar -czf znz001trn.tar.gz data\n",
    "\n",
    "# #save to drive\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fast.ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !curl https://course.fast.ai/setup/colab | bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "```text\n",
      "=== Software === \n",
      "python       : 3.7.6\n",
      "fastai       : 1.0.60\n",
      "fastprogress : 0.2.2\n",
      "torch        : 1.3.1\n",
      "torch cuda   : None / is **Not available** \n",
      "\n",
      "=== Hardware === \n",
      "No GPUs available \n",
      "\n",
      "=== Environment === \n",
      "platform     : Darwin-17.7.0-x86_64-i386-64bit\n",
      "conda env    : solaris\n",
      "python       : /Users/brentsair/anaconda3/envs/solaris/bin/python\n",
      "sys.path     : /Users/brentsair/Documents/Flatiron_School.nosync/SegmentingBuildings/notebooks/exploratory\n",
      "/usr/local/Cellar/apache-spark/2.4.4/libexec/python\n",
      "/python\n",
      "/Users/brentsair/Documents/Flatiron_School.nosync/SegmentingBuildings/notebooks/exploratory\n",
      "/Users/brentsair/anaconda3/envs/solaris/lib/python37.zip\n",
      "/Users/brentsair/anaconda3/envs/solaris/lib/python3.7\n",
      "/Users/brentsair/anaconda3/envs/solaris/lib/python3.7/lib-dynload\n",
      "\n",
      "/Users/brentsair/anaconda3/envs/solaris/lib/python3.7/site-packages\n",
      "/Users/brentsair/anaconda3/envs/solaris/lib/python3.7/site-packages/IPython/extensions\n",
      "/Users/brentsair/.ipython\n",
      "no supported gpus found on this system\n",
      "```\n",
      "\n",
      "Please make sure to include opening/closing ``` when you paste into forums/github to make the reports appear formatted as code sections.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from fastai.vision import *\n",
    "import fastai.callbacks  \n",
    "from fastai.utils.collect_env import *\n",
    "show_install(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "BS_dataprep.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Solaris",
   "language": "python",
   "name": "solaris"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
